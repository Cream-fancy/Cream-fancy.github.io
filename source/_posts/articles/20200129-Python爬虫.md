---
title: 基于 Python 爬虫
date: 2020-01-29 17:52:34
updated: 2020-02-03 10:57:15
author: Forrest
tags:
- 爬虫
---

## 1. 内容简介

### 1.1 爬虫介绍

> 爬虫是编写程序，去互联网上抓取数据的过程。

**那些语言可以爬虫**

- PHP，多进程、多线程支持不好
- Java，代码臃肿，重构成本大
- C, C++，直接和底层接触
- Python，支持第三方模块，强大的框架 Scrapy

**爬虫分类**

- 通用爬虫

<div class="bigbox">

一般是搜索引擎，如百度，360，必应

工作流程

1. 抓取网页
2. 数据采集
3. 数据处理
4. 提供检索

如何爬取新网站

1. 主动提交 URL
2. 设置友情链接
3. 和 DNS 服务商合作

搜索结果如何排名

1. 竞价排名
2. 根据 pagerank 值，如访问量、点击量

爬取限制

1. 网站跟目录下的 robots.txt 告诉搜索引擎哪些内容不可以获取
2. robots 协议不是规范而是约定俗成，并不能保证网站的隐私

</div>


- 聚焦爬虫

<div class="bigbox">

只抓取特定的信息，代替浏览器上网的脚本

可爬取的网页有什么特点？

1. 唯一的 URL
2. 内容为 HTML 结构
3. 使用 http、https 协议

爬取的步骤是什么？

1. 获取网页的 URL
2. 写程序向 URL 发送请求，获取响应
3. 解析页面的内容，提取有用的数据

</div>

### 1.2 笔记概要

**使用的第三方库**

```
urllib
requests
bs4
```

**解析网页内容**

```
RegExp
bs4
xpath
jsonpath
```

**动态模拟**

```
selenium + phantomjs
chromeheadless
```

**高性能框架**

```
Scrapy
PySpider
```

**分布式爬虫**

```
scrapy-redis 组件
```

**反爬虫 & 反反爬虫**

- 伪造 UA（浏览器类型）
- 正向代理（封掉你的 IP 就需要找代理）
- 验证码（某些网页访问量过大会给验证码进行识别）
- 动态页面（执行 js 文件才加载的页面）


### 1.3 Web 简介

![Web 工作原理图](https://s4.51cto.com/images/blog/201808/29/8b2a93d37889d13e67f94ece57082033.jpg?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)

**协议分析**

常见端口号

```
http(80)
https(443)
ftp(21)

ssh(22)
mysql(3306)
redis(6379)
mongo(27017)
```

http 与 https 的区别

```
http 是超文本传输协议，信息是明文传输
https 则是具有安全性的 ssl 加密传输协议

对称加解密：加密与解密的秘钥是一样的
非对称加解密：公钥和私钥是成对出现的，公钥加密私钥解密，私钥加密公钥解密
配置 ssh 会用到，搭建主机信任
```

http 请求与响应

```
http://[host]:[port]/[path]?[query string]#[anchor]

http 请求：请求行、请求头、请求内容
    请求行 GET / POST ...
    请求头 请求细节信息，需要掌握
    
http 响应：状态行、响应头、响应内容
    状态行 状态码，表示服务器对请求的处理结果
    响应头 响应细节信息，只需理解
```

常见请求头

```
User-Agent: 浏览器类型
Accept-Encoding: 支持的压缩格式
Referer: 指示客户端从哪个页面跳转进来的，用于网站的防盗链
```

常见响应头

```
Content-Encoding: 数据的压缩格式
Content-Type: 回送的数据类型
```

常见状态码

```
200: 请求成功，获得响应内容

301: 请求到的资源会分配一个永久的 URL（永久重定向）
302: 请求到的资源在一个不同的 URL 处临时保存，如浏览器缓存（临时重定向）

403: 拒绝请求。丢弃
404: 没有找到，丢弃

500: 服务器内部错误
```

### 1.4  抓包工具

**浏览器**

<div class="bigbox">

进入开发者模式

```
Elements 网页代码
Console  调试 JavaScript 的控制台
Network  浏览器和服务器交互的过程
```

一个网页的呈现，中间不止一次 http 请求

```
XHR 页面的 AJAX 请求
js  所有 js 文件都要单独发送一个请求
```

请求右边栏的详细信息

```
Headers
    General：简单信息
    Response Headers
    Request Headers
    Query String：GET 参数
    Form Data：POST 参数
Response
```

</div>

**2. Fiddler**

<div class="bigbox">

配置

```
tools -> options -> https
capture -> decrypt -> ingnore
actions -> trust
```

抓包

```
file ==> capture
```

类型

```
<>:     html 内容
{json}: json 数据，很有可能是前端和后端的接口
{css}:  CSS 文件
{js}:   JavaScript 文件
```

分析 Inspectors

```
请求信息
    Raw:        请求头信息
    WebForms:   请求所带参数，query string / form data
响应信息
    Raw:        响应信息
    JSON / XML: 接口返回信息
```

快捷代码

```
clear:          清除请求
select json:    选择 json 请求
select image:   选择图片
?xxx:           包含<xxx>的请求
```

fiddler 对 Android 应用抓包设置

https://zhinan.sogou.com/guide/detail/?id=1610001831
https://blog.csdn.net/majinggogogo/article/details/72974441
</div>

## 2. urllib

### 2.1 urllib 简介

**什么是 urllib**

```python
python2: urllib / urllib2
python3: urllib.request / urllib.parse
```

**urllib.request 库**

```python
urlopen()       # 发送请求
urlretrieve()   # 直接将 url 下载到本地
```

**urllib.parse 库**

```python
quote()         # URL 编码函数
unquote()       # URL 解码函数
urlencode()     # 传入字典参数，编码成 query 格式
```

**response 对象**

```python
read()          # 请求内容，返回 bytes 类型
geturl()        # 获取请求的 URL
getheaders()    # 获取响应头，返回列表中有元组的形式[(),(),...]
getcode()       # 获取状态码
readlines()     # 按行读取，返回列表，元素为字节类型，很少用到
```

### 2.2 urllib.request

**什么是 URL**

<div class="bigbox">

<center>URL (Uniform Resource Locator)</center>

URL 称为统一资源定位符，在互联网上传输时会将 *保留的，不可打印的，非 ASCII* 的字符，进行编码或转义成 *%xx (followed by two hexadecimal digits)*

</div>

**网页的编码**

<div class="bigbox">

<center>编码的发展</center>

- 美国提出 ASCII 编码，基础ASCII码7位共 128 种字符，扩展ASCII码8位共 256 种字符
- 传入中国之后，由于汉字太多，ASCII 表不能满足，便扩展成 GB2312，后来亚洲的字符收录成 GBK
- 各个国家都按照自己语言对 ASCII 进行扩展，造成了编码的混乱，使得国际之间难以交流。国际提出构造 Unicode 编码，所有字符都用两个字节进行表示
- 但美国不乐意了，ASCII 本来只占一个字节，但 Unicode 白白浪费了一个字节，于是通过协商出现了通用转换格式 (Unicode Transformation Format)，常见的有 utf-8 和 utf-16。其中 utf-8 规定美国 1 个字节，欧洲 2 个字节，东南亚 3 个字节

</div>

编码 (encode) 与解码 (decode)

```python
encode(encoding='utf-8')    # 编码成 Unicode
decode(encoding='utf-8')    # Unicode 解码
```

```py
[str].encode('gbk') # 将字符串按 gbk 编码成 Unicode
[bytes].decode()    # 将字节数据解码成 utf-8
```

**使用 request 库发送请求**

```py
# 向 url 发送请求，data 是参数，默认发送GET请求，传入参数则发送POST请求
urlopen(url, data=None)

# 将 url 资源下载到本地 filename 路径并命名，常用于下载图片
urlretrieve(url, filename=None, ..., data=None)
```

**2.4 例子**

编码与解码

```python
str1 = '中文'
print(type(str1), str1)

data = str1.encode('gbk')
print(type(data), data)

str1 = data.decode()
print(type(str1), str1)

"""
UnicodeDecodeError: 'utf-8' codec can't decode byte ...
<class 'str'> 中文
<class 'bytes'> b'\xd6\xd0\xce\xc4'
"""
```

请求与响应
```python
import urllib.request

url = 'http://www.baidu.com'

response = urllib.request.urlopen(url)

# 写入文件
with open('baidu.html', 'w', encoding='utf8') as fp:
    fp.write(response.read().decode())

# with open('baidu.html', 'wb') as fp:
#     fp.write(response.read())

print(response.geturl())
print(response.getcode())
print(dict(response.getheaders()))

"""
>>> http://www.baidu.com
>>> 200
>>> {'Bdpagetype': '1', 'Bdqid': '0x9b349e12002c208e', ... }
"""
```

下载图片

```python
import urllib.request

image_url = 'https://ps.ssl.qhimg.com/sdmt/232_135_100/t01f49731f2b8f45561.jpg'

response = urllib.request.urlopen(image_url)

# 图片只能以二进制写入
with open('beauty1.jpg', 'wb') as fp:
    fp.write(response.read())

# 也可以用 urlretrieve 下载图片
urllib.request.urlretrieve(image_url, 'beauty2.jpg')
```

### 2.3 urllib.parse

**parse 库编码与解码**

```py
# 将 string 编码为安全的格式，safe 规定了那些字符是安全的，不会对其进行编码
quote(string, safe='/')

# 将 string 解码为特定编码的字符，默认是 utf-8
unquote(string, encoding='utf-8')

# 将 key-value 格式的 query 编码成 query_string 格式
urlencode(query)
```


**例子**

```python
import urllib.parse

string = 'http://www.baidu.com?wd=编码&t=123'

url = urllib.parse.quote(string)    # 编码
print(url)

string = urllib.parse.unquote(url)  # 解码
print(string)

"""
>>> http%3A//www.baidu.com%3Fwd%3D%E7%BC%96%E7%A0%81%26t%3D123
>>> http://www.baidu.com?wd=编码&t=123
"""
```

```python
import urllib.parse

# QueryString 为 name=Cream&sex=女&age=18
url = 'http://www.baidu.com/index.html/'

data = {
    'name': 'Cream',
    'sex': '女',
    'age': 18,
}

# 编码
query_string = urllib.parse.urlencode(data)
print(query_string)

# 拼接 URL
url = url + '?' + query_string
print(url)

"""
>>> name=Cream&sex=%E5%A5%B3&age=18
>>> http://www.baidu.com/index.html/?name=Cream&sex=%E5%A5%B3&age=18
"""
```

### 2.4 GET & POST

**GET请求**

获取百度搜索内容

```python
word = input("请输入您要搜索的内容...")
url = 'http://www.baidu.com/s?'

# 写入参数
data = {
    'ie': 'utf-8',
    'wd': word,
}

url += urllib.parse.urlencode(data)

response = urllib.request.urlopen(url)

filename = word + '.html'

with open(filename, 'wb') as fp:
    fp.write(response.read())
```

伪装 User-Agent，让服务端认为是浏览器在访问

<div class="bigbox">

<center>常见的浏览器 UA</center>

Chrome｜谷歌浏览器 `Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.87 
Safari/537.36`

Firefox｜火狐浏览器 `Mozilla/5.0 (Windows NT 6.1; WOW64; rv:46.0) Gecko/20100101 
Firefox/46.0`

</div>

```python
import urllib.request

url = 'http://www.baidu.com/'

# 构造请求头
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.87 Safari/537.36'
}

# 构建请求对象
request = urllib.request.Request(url=url, headers=headers)

print(request.get_full_url())   # 查看请求的完整 url
print(request.get_method())     # 查看请求的方法

# 发送请求
response = urllib.request.urlopen(request)

print(response.info())
print(response.read().decode())

"""
>>> http://www.baidu.com/
>>> GET
>>> ...
"""
```

**POST 请求**

百度翻译 API

```python
import urllib.request

post_url = 'https://fanyi.baidu.com/sug'
word = input("请输入你要查询的单词...")
form_data = {
    'kw': word,
}

# 创建请求
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36 SE 2.X MetaSr 1.0',
}
request = urllib.request.Request(url=post_url, headers=headers)

# 处理表单数据 dict ==> string ==> bytes
form_data = urllib.parse.urlencode(form_data).encode()

# 传入 data 参数
response = urllib.request.urlopen(request, data=form_data)

# 得到 json 数据
print(response.read().decode())
```

复杂类型的API，为了防爬 PC 端接口优化，下面是接口分析
https://www.jianshu.com/p/38a65d8d3e80

```python
import urllib.request

post_url = 'http://fanyi.baidu.com/v2transapi'

form_data = {
    'from': 'en',
    'to': 'zh',
    'query': 'baby',
    'transtype': 'realtime',
    'simple_means_flag': '3',
    'sign': '814534.560887',
    'token': '9fcc07abb0eaf0a201b18bca32be2fb8',
}

hearders = {
    'Accept': '*/*',
    # 'Accept-Encoding':'gzip, deflate, br',  # 浏览器可解压缩，但 Python 不能
    'Accept-Language': 'zh-CN,zh;q=0.8',
    'Connection': 'keep-alive',
    # 'Content-Length': '121',  # 省去
    'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
    'Cookie': 'BDUSS=H5ONHc2MmlSTmthWUVxM2NxS0lKQUxEYmsxWTRuZnQ4SW5DcUZYN20zWER5UXhjQVFBQUFBJCQAAAAAAAAAAAEAAADaVKda0Me~1V~E6ruqAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMM85VvDPOVbS; PSTM=1548227996; BAIDUID=FFD370C5397C97558351CED5622D7DE7:FG=1; BIDUPSID=DF35616CD84A34C386AE3A7021FBD835; BDORZ=B490B5EBF6F3CD402E515D22BCDA1598; locale=zh; to_lang_often=%5B%7B%22value%22%3A%22en%22%2C%22text%22%3A%22%u82F1%u8BED%22%7D%2C%7B%22value%22%3A%22zh%22%2C%22text%22%3A%22%u4E2D%u6587%22%7D%5D; REALTIME_TRANS_SWITCH=1; FANYI_WORD_SWITCH=1; HISTORY_SWITCH=1; SOUND_SPD_SWITCH=1; SOUND_PREFER_SWITCH=1; Hm_lvt_64ecd82404c51e03dc91cb9e8c025574=1548302954,1548304309; Hm_lpvt_64ecd82404c51e03dc91cb9e8c025574=1548304309; from_lang_often=%5B%7B%22value%22%3A%22zh%22%2C%22text%22%3A%22%u4E2D%u6587%22%7D%2C%7B%22value%22%3A%22en%22%2C%22text%22%3A%22%u82F1%u8BED%22%7D%5D',
    'Host': 'fanyi.baidu.com',
    'Origin': 'https://fanyi.baidu.com',
    'Referer': 'https://fanyi.baidu.com/',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36 SE 2.X MetaSr 1.0',
    'X-Requested-With': 'XMLHttpRequest',
}

request = urllib.request.Request(url=post_url, data=form_data)

form_data = urllib.parse.urlencode(form_data).encode()

response = urllib.request.urlopen(request, form_data)

print(response.read().decode())

"""
>>> urllib.error.HTTPError: HTTP Error 401: Unauthorized
"""
```

### 2.5 ajax

**什么是 ajax**

Asynchronism JavaScript And XML，是一种用于创建快速动态网页的技术。可以在不重新加载整个网页的情况下，对网页的某部分进行更新。

比如当您在谷歌的搜索框输入关键字时，JavaScript 会把这些字符发送到服务器，然后服务器会返回一个搜索建议的列表。

**ajax get**

豆瓣电影

```python
import urllib.request
import urllib.parse

# https://movie.douban.com/j/search_subjects?type=movie&tag=%E7%83%AD%E9%97%A8&sort=recommend&page_limit=20&page_start=20
# page_start 表示从第几条记录开始，page_limit 表示接下来显示几条数据
url = 'https://movie.douban.com/j/search_subjects?type=movie&tag=%E7%83%AD%E9%97%A8&sort=recommend&'

page = int(input("请输入想要查看页码..."))
num = 20

# 构建 get 参数
data = {
    'page_start': (page - 1) * num,
    'page_limit': num,
}
query_string = urllib.parse.urlencode(data)
url += query_string

# 构建请求
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.87 Safari/537.36'
}
request = urllib.request.Request(url=url, headers=headers)

# 返回 json 数据
response = urllib.request.urlopen(request)
print(response.read().decode())
```

**ajax post**

KFC

```python
import urllib.request
import urllib.parse

url = 'http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=pid'

city = input("请输入要查询的城市...")
page = input("请输入要查询的页码...")
size = input("请输入要显示的记录条数...")

form_data = {
    'cname': city,
    'pid': '20',
    'pageIndex': page,
    'pageSize': size,
}

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.87 Safari/537.36'
}

request = urllib.request.Request(url=url, headers=headers)
form_data = urllib.parse.urlencode(form_data).encode()

response = urllib.request.urlopen(url=url, data=form_data)
print(response.read().decode())

```

百度贴吧

输入吧名，起始页码，结束页码，在当前文件夹中创建一个以吧名命名的文件夹，里面是每一页 html 的内容，文件名是 *name_page.html*

```python
import urllib.request
import urllib.parse
import os

# http://tieba.baidu.com/f?ie=utf-8&kw=穷游&red_tag=x2499331124  第一页
# http://tieba.baidu.com/f?kw=%E7%A9%B7%E6%B8%B8&ie=utf-8&pn=50  第二页
# http://tieba.baidu.com/f?kw=%E7%A9%B7%E6%B8%B8&ie=utf-8&pn=100  第三页

url = 'http://tieba.baidu.com/f?ie=utf-8&'

name = input("请输入吧名...")
page_start = int(input("请输入起始页码..."))
page_end = int(input("请输入结束页码..."))

# 创建文件夹
if not os.path.exists(name):
    os.mkdir(name)

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.87 Safari/537.36'
}

# 依次爬取每一页的数据
for page in range(page_start, page_end+1):  # 左闭右开
    data = {
        'kw': name,
        'pn': (page - 1) * 50,
    }
    param = urllib.parse.urlencode(data)
    request = urllib.request.Request(url=url+param, headers=headers)

    print('第%s页开始下载...' % page)
    response = urllib.request.urlopen(request)

    file_path = str.format("{0}/{1}_{2}.html", name, name, page)
    with open(file_path, 'wb') as fp:       # 'wb' 格式写入文件会自动清空
        fp.write(response.read())

print('下载完毕')
```

### 2.6 其他用法 & 常见问题

**编码问题**

```python
import requests

url = "http://www.mmonly.cc/mmtp/qcmn/294152.html"

r = requests.get(url)
r.encoding = "gb2312"

print(r.text)
```

**SSL**

```python
import urllib.request
import ssl

url = "https://bbs.csdn.net/"
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64; rv:65.0) Gecko/20100101 Firefox/65.0'
}

ssl._create_unverified_context()  # 忽略证书

request = urllib.request.Request(url, headers=headers)
response = urllib.request.urlopen(request)
print(response.read().decode())
```

**Proxy**

什么是代理？

<div class="bigbox">

Proxy Server，其功能就是代理网络用户去取得网络信息

- 正向代理

代理客户端获取数据

客户端频繁向服务端发送请求，没有时间间断，服务端会认为在爬取数据，封掉客户端的 IP，找代理，让代理代替你去访问服务端，然后回传数据。维护一个代理池，不停变换代理，服务端就不知道封哪个 IP

- 反向代理

代理服务端提供数据

例如百度服务器，在全国各地都有子服务器，客户端提交请求会传给子服务器处理，加快响应的速度

- 按匿名程度分类

透明代理：对方服务器可以知道你使用了代理，并且知道你的真实IP
匿名代理：对方服务器可以知道你使用了代理，但不知道你的真实IP
高匿名代理：对方服务器不知道你使用了代理，更不知道你的真实IP

</div>

如何设置代理？

浏览器自带（Firefox, Chrome...），第三方软件（Shadowsocks...）
Python 爬虫 `proxy = {'http': '<ip>:<port>'}`，用 `opener.open()` 发送请求

Handler 的用法

```python
""" Request & Response
"""

import urllib.request

url = 'http://www.baidu.com/'

# 创建 handler
handler = urllib.request.HTTPHandler()
# 创建 opener
opener = urllib.request.build_opener(handler)
# 构建请求对象
request = urllib.request.Request(url=url)
# opener 发送请求
response = opener.open(request)

print(response.getcode())
```

```python
""" Redirect
"""

import urllib.request

url = "http://www.baidu.cn"

response = urllib.request.urlopen(url)

print(response.geturl())
```

```python
""" Redirect Handler
"""

import urllib.request


class RedirectHandler(urllib.request.HTTPRedirectHandler):
    # 302 重定向
    def http_error_302(self, req, fp, code, msg, headers):
        # 重定向之后的 url
        response = urllib.request.HTTPRedirectHandler.http_error_301(self, req, fp, code, msg, headers)
        # 状态码
        response.status = code
        # 重定向的地址
        response.newUrl = response.geturl()
        print(response.newUrl, response.status)
        # 返回给 opener
        return response


opener = urllib.request.build_opener(RedirectHandler)
opener.open("http://www.baidu.cn")
```

```python
""" Proxy
"""

import urllib.request

# proxy
proxy = {'http': '113.79.75.104:9797'}
# handle
handler = urllib.request.ProxyHandler(proxy)
# opener
opener = urllib.request.build_opener(handler)

url = 'http://httpbin.org/get'

request = urllib.request.Request(url=url)

response = opener.open(request)

print(response.read().decode())
```

代理网站

https://www.kuaidaili.com/
https://www.xicidaili.com/

**Cookie**

<div class="bigbox">

<center>Cookie 是什么</center>

http 是无状态的，http 一次请求一次响应，然后断开，难以实现动态交互，而 Cookie 和 Session 就是两种用于保持 http 连接状态的技术。
Cookie 是服务端给客户端保存在本地的特殊信息，常用于模拟登陆。

</div>

抓包截获 Cookie，但是 Cookie 是会更新的

```python
import urllib.request

url = 'http://www.renren.com/969543945/profile'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36 SE 2.X MetaSr 1.0',
    'Cookie': 'anonymid=joogr9btrbqmql; depovince=GX; _r01_=1; JSESSIONID=abc9PML-EqqVN5u-xDaIw; ick_login=02ed2324-0993-4d86-976b-8302e1d2257f; t=8ebcd1646809435f8c738d2f07c40fd85; societyguester=8ebcd1646809435f8c738d2f07c40fd85; id=969543945; xnsid=7f8a53fb; jebecookies=6290e1f2-c883-40d1-b4b6-34eeec7fd735|||||; ver=7.0; loginfrom=null; jebe_key=cc3f4f1b-397a-4000-be6b-c211c9231fe3%7Cf83bca49d1e6d3aa628d5a6b3adaaa71%7C1548343424127%7C1%7C1548343425666; wp_fold=0'
}

request = urllib.request.Request(url=url, headers=headers)

response = urllib.request.urlopen(request)

with open('Profile.html', 'wb') as fp:
    fp.write(response.read())
```

人人网

```python
import urllib.request
import urllib.parse
import http.cookiejar

# 创建 cookiejar 对象
cj = http.cookiejar.CookieJar()
# 通过 cookiejar 创建 handler
handler = urllib.request.HTTPCookieProcessor(cj)
opener = urllib.request.build_opener(handler)

post_url = 'http://www.renren.com/ajaxLogin/login?1=1&uniqueTimestamp=2019051058783'
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) '
    'Chrome/58.0.3029.110 Safari/537.36 SE 2.X MetaSr 1.0',
}
form_data = {
    'email':        '18260989305',
    # 'icode':
    'origURL':      'http://www.renren.com/home',
    'domain':       'renren.com',
    'key_id':       '1',
    'captcha_type': 'web_login',
    'password':     '41a3fcd8e1a39fb8b7207a9cce850017d861281ce9126e20a5d66f8e94f41274',
    'rkey':         '9f122045ee68227cb5e2b1c9612a13ed',
    'f':            'https%3A%2F%2Fwww.sogou.com%2Flink%3Furl%3DDSOYnZeCC_r-h4h4RsVJtWYvcrgiTSe0',
}
form_data = urllib.parse.urlencode(form_data).encode()
request = urllib.request.Request(url=post_url, headers=headers)
# response = urllib.request.urlopen(request, data=form_data)
response = opener.open(request, data=form_data)  # 所有的请求都要由 opener 发送
print(response.read().decode())

# 但这时候还不能直接访问个人中心，需要 cookie
# 需要模拟浏览器 post 请求后，将 cookie 保存下来
# 需要用 opener 保存 cookie，之后的请求 opener 会携带 cookie 过去

get_url = 'http://www.renren.com/969543945/profile'
request = urllib.request.Request(url=get_url, headers=headers)
response = opener.open(request)
with open('人人网.html', 'wb') as fp:
    fp.write(response.read())
```

**Exception**

```
- Python 常见异常
NameError
TypeError
FileNotFind

- 异常处理
try - except

- HTTP 常见异常
URL Error
    （1）没有网
    （2）服务器连接失败
    （3）找不到指定的服务器
HTTP Error
    是 URLError 的子类
```

```python
""" URL Error
"""

import urllib.request
import urllib.error

url = 'http://ooxx.com/'

# 捕获 URL Error
try:
    response = urllib.request.urlopen(url)
    print(response)
except urllib.error.URLError as e:  # Exception 可以捕获任何异常
    print(e)

"""
>>> <urlopen error [Errno 11002] getaddrinfo failed>
"""
```

```python
""" HTTP Error
"""

import urllib.request
import urllib.error

# https://blog.csdn.net/kobebryantlin0/article/details/73391584 将最后一个字符删去
url = 'https://blog.csdn.net/kobebryantlin0/article/details/7339158'

# 捕获 HTTP Error
try:
    response = urllib.request.urlopen(url)
    print(response)
except urllib.error.HTTPError as e:  # 先用 HTTP Error 尝试捕获
    print(e)
    print(e.code)
except urllib.error.URLError as e:  # 父类后面捕获
    print(e)

"""
>>> HTTP Error 404: Not Found
>>> 404
"""
```

**Debug**

```python
import urllib.request

# 访问网络，输出调试信息
httpHandler = urllib.request.HTTPHandler(debuglevel=1)
httpsHandler = urllib.request.HTTPSHandler(debuglevel=1)

# 全局生效
opener = urllib.request.build_opener(httpHandler, httpsHandler)
urllib.request.install_opener(opener)

urllib.request.urlopen("http://www.baidu.com")

"""
>>> send: b'GET / HTTP/1.1\r\nAccept-Encoding: identity\r\nHost: www.baidu.com\r\nUser-Agent: Python-urllib/3.8\r\nConnection: close\r\n\r\n'
>>> reply: 'HTTP/1.1 200 OK\r\n'
>>> header: Bdpagetype: 1
>>> ...
"""
```

## 3. requests

### 3.1 requests 简介

**什么是 requests**

requests 是对 urllib 的封装，使用起来更简单。

**安装**

```
pip install requests
```

**官方文档**

http://cn.python-requests.org/zh_CN/latest/

**get 请求**

```python
r = requests.get(url, headers=headers, params=data)
```

**post 请求**

```python
r = requests.post(url, headers=headers, data=form_data)
```

**response 对象**

```
r.text          字符串形式查看响应
r.content       字节形式查看相应
r.headers       查看响应头
r.url           查看请求的 url
r.status_code   查看状态码
r.encoding      查看或设置编码类型
r.json()        查看 json 格式数据
```

**Proxy**

```python
r = requests.get(url, headers=headers, proxies=proxies)
```

**Cookie**

```python
s = requests.Session()  # 用 Session 请求 s.get(), s.post()
```

### 3.2 GET & POST

**GET 请求**

百度搜索

```python
import requests

url = 'http://www.baidu.com/s'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:46.0) Gecko/20100101 Firefox/46.0'
}
data = {
    'wd': '美女',
}

# parameters 参数
r = requests.get(url, headers=headers, params=data)

print(r.headers)        # 响应头
print(r.url)            # 所请求的 url
print(r.status_code)    # 状态码
print(r.encoding)       # 编码类型
r.encoding = 'utf8'     # 指定编码类型

# 查看字符串类型
print(type(r.text))
# 查看字节类型
with open('Baidu.html', 'wb') as fp:
    fp.write(r.content)
```

下载图片

```python
import requests

url = "https://timgsa.baidu.com/timg?quality=80&size=b9999_10000&sec=1552218997352"
src = "&src=http%3A%2F%2Fs7.sinaimg.cn%2Fmw690%2F001m1Utdzy6ZLnVyRxQe6%26690"
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64; rv:65.0) Gecko/20100101 Firefox/65.0',
}

r = requests.get(url + src, headers=headers, stream=True)
with open('Beauty.png', 'wb') as fp:
    for chunk in r:
        fp.write(chunk) # 一块一块写入数据
```

**POST 请求**

必应翻译

```python
import requests

url = 'http://cn.bing.com/tlookupv3?isVertical=1&&IG=30093A230F804A2A8F0F5E4F9958367F&IID=translator.5028.3'

headers = {
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64; rv:65.0) Gecko/20100101 Firefox/65.0'
}

form_data = {
    'from': 'en',
    'to': 'zh-Hans',
    'text': 'sexy',
}

r = requests.post(url, headers=headers, data=form_data)

print(r.json())
```

### 3.3 反反爬虫

**Session**

<div class="bigbox">

<center>什么是 Session</center>

Session 在服务端存储特定用户会话所需的属性及配置信息。

当用户在应用程序的网页之间跳转时，Session 在整个用户会话中一直存在下去。通过 SessionID 判断身份，是依赖于 Cookie 的

</div>

人人网

```python
import requests

# 创建会话
s = requests.Session()

post_url = 'http://www.renren.com/ajaxLogin/login?1=1&uniqueTimestamp=2019041955786'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64; rv:65.0) Gecko/20100101 Firefox/65.0'
}

form_data = {
    'email': '18260989305',
    'icode': '',
    'origURL': 'http://www.renren.com/home',
    'domain': 'renren.com',
    'key_id': '1',
    'captcha_type': 'web_login',
    'password': '55a4690423ba5b10ccf22da83e58469df5acd7be4f7800c07a88fca7904704a6',
    'rkey': 'ccac553351d9d23d175455f1a2097e27',
    'f': 'http%3A%2F%2Fwww.renren.com%2F969544246',
}

# 通过 Session 访问，s 会保存 Cookie 信息
r = s.post(post_url, headers=headers, data=form_data)

get_url = 'http://www.renren.com/969543945/profile'

r = s.get(get_url, headers=headers)

with open('renren.html', 'wb') as fp:
    fp.write(r.content)
```

**Proxy**

```python
import requests

url = 'http://httpbin.org/ip'

# 设置代理
proxies = {
    'http': 'http://223.199.24.230:9999',
}

r = requests.get(url, proxies=proxies)

print(r.text)
```

**Token**

<div class="bigbox">

<center>表单令牌是什么</center>

表单令牌用于防止数据的重复提交。

用户在访问表单时，会随机生成一个 token 保存在 Session 中，提交表单会携带 token 值一起给服务器，服务器判断此时的 token 与缓存的是否一致，来返回提交成功或失败

</div>

ChinaUnix

```python
import requests

# ChinaUnix 只需要用户编号就能访问该用户的空间
# http://bbs.chinaunix.net/home.php?mod=space&uid=69908989&do=profile

user_url = 'http://bbs.chinaunix.net/home.php?mod=space&do=profile&uid={}'

no = input("请输入你想查看的用户编号...")

user_url = user_url.format(no)

r = requests.get(user_url)

with open('Space.html', 'wb') as fp:
    fp.write(r.content)
```

ChinaUnix 模拟登陆

```python
import requests
from bs4 import BeautifulSoup

get_url = 'http://account.chinaunix.net/login/?url=http%3A%2F%2Fbbs.chinaunix.net%2F'

headers = {
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64; rv:65.0) Gecko/20100101 Firefox/65.0'
}

r = requests.get(get_url)

# bs4 解析
soup = BeautifulSoup(r.content, 'lxml')
# 通过属性选择器找到 token 值
token = soup.select('input[name="_token"]')[0]['value']

post_url = 'http://account.chinaunix.net/login/login'

form_data = {
    'username': 'monkey2019',
    'password': '1998o713shan',
    '_token': token,
    '_t': '1548936053660',
}

r = requests.post(post_url, headers=headers, data=form_data)

print(r.text)
```

**验证码**

## 4. Regular Expression

### 4.1 Reg 简介

**什么是正则表达式**

正则表达式通常被用来检索、替换符合某个规则的文本。

### 4.2 Reg 语法

**单字符**

```
.   任意非换行符，与 re.S 使用能匹配任意字符
[]  集合内任意字符
\d  数字
\D  非数字
\w  数字、字母、下划线、中文
\W  非 \w
\s  空白符
\S  非空白符
```

**重复修饰符**

```
*   匹配 0 到任意次
+   匹配 1 到任意次
?   匹配 0 到 1 次
{m}     匹配 m 次
{m,}    匹配 m 到任意次
{m,n}   匹配 m ~ n 次
```

**边界修饰符**

```
^   字符串的开头，与 re.M 使用还能匹配换行符的后一个字符
$   字符串尾，与 re.M 使用还能匹配换行的前一个字符
```

**分组**

```
(...)   括号内容视为一个整体来匹配和获取，并能被 \number 再次匹配
\1 \2   子模式，匹配数字代表的组合，组合从 1 编号
```

**贪婪模式 & 懒惰模式**

```
* + ?     贪婪模式，会匹配尽可能多的字符
*? +? ??  取消贪婪，匹配尽量少的字符
```

### 4.2 python 中使用正则

**正则表达式**

```
r'...'
```
正则表达式中使用 `'\'` 来表示特殊形式，而 Python 中也有转义的作用，这就产生了冲突。
解决办法是通过使用前缀 `'r'`，使字符串中的反斜杠不在 Python 中转义，而保持原始字符串。

**re 模块**

```
re.I    IGNORECASE 忽略大小写
re.M    MULTILINE 多行匹配，影响 ^ $
re.S    DOTALL 单行匹配，影响 .

re.compile(pat)         生成正则对象
re.match(pat, str)      生成从开始位置匹配的正则对象
re.search(pat, str)     生成从任意位置匹配的正则对象
re.findall(pat, str)    以列表返回互不相交的匹配结果
re.sub(pat, repl, str)  返回替换后的字符串
```

**正则对象**

```
pat.match(str)      生成从开始位置匹配的正则对象
pat.search(str)     生成从任意位置匹配的正则对象
pat.findall(str)    以列表返回互不相交的匹配结果
pat.sub(repl, str)  返回替换后的字符串
```

**分组**

```python
import re

string = '<p><div><span>北京欢迎你</span></div></p>'

# 需求：匹配 <><>...</></> 并且前后标识一致
pattern = re.compile(r'<(\w+)><(\w+)>\w+</\2></\1>')

res = pattern.search(string)

print(res)

"""
>>> <re.Match object; span=(3, 32), match='<div><span>北京欢迎你</span></div>'>
"""
```

**贪婪模式 & 懒惰模式**

```python
import re

string = '<p>北京欢迎你</p></p></p>'

pat1 = re.compile(r'<p>.*</p>')     # 贪婪
pat2 = re.compile(r'<p>.*?</p>')    # 懒惰

print(pat1.search(string))
print(pat2.search(string))

"""
>>> <re.Match object; span=(0, 20), match='<p>北京欢迎你</p></p></p>'>
>>> <re.Match object; span=(0, 12), match='<p>北京欢迎你</p>'>
"""
```

**边界修饰 & 多行匹配**

```python
string = '''apple#banana
apple#banana
'''

print(re.findall(r'^apple', string))
print(re.findall(r'^apple', string, re.M))

"""
>>> ['apple']
>>> ['apple', 'apple']
"""
```

```python
string = 'apple#banana\napple#banana'

pat1 = re.compile(r'banana$')
print(pat1.findall(string))

pat2 = re.compile(r'banana$', re.M)
print(pat2.findall(string))

"""
>>> ['banana']
>>> ['banana', 'banana']
"""
```

```python
import re

string = '''loving self
love myself
love yourself
'''

pat1 = re.compile(r'^love')
pat2 = re.compile(r'^love', re.M)

print(pat1.search(string))
print(pat2.search(string))
print(pat2.findall(string))

"""
>>> None
>>> <re.Match object; span=(12, 16), match='love'>
>>> ['love', 'love']
"""
```

**单行匹配**

```python
import re

string = '''<div>
君不见，黄河之水天上来，奔流到海不复回。
君不见，高堂明镜悲白发，朝如青丝暮成雪。
</div></div>
'''

pattern = re.compile(r'<div>(.*?)</div>', re.S)

res = pattern.findall(string)[0]

print(repr(res))

"""
>>> '\n君不见，黄河之水天上来，奔流到海不复回。\n君不见，高堂明镜悲白发，朝如青丝暮成雪。\n'
"""
```

**match & search**

```python
import re

string = 'In the 2020s, I know a website "http://wx1.sinaimg.cn/mw600/0076BSS5ly1gbfterhmrbj30m80x3ju6.jpg".'

pattern = re.compile(r'\d{4}s')

print(pattern.match(string))
print(pattern.search(string))

"""
>>> None
>>> <re.Match object; span=(7, 12), match='2020s'>
"""
```

**findall**

所有匹配结果互不相交。

第一次匹配 `'abc  def'`，接着往后匹配 `'ghi jkl'`

```python
string = 'abc  def  ghi  jkl'
print(re.findall(r'\w+\s+\w+', string))

"""
>>> ['abc  def', 'ghi  jkl']
"""
```

若正则中有一个括号，返回的不是整个正则的匹配结果，而是该括号匹配的内容，最后以列表形式返回。

匹配结果同上，括号在第一次匹配中对应 `'abc'`，在第二次匹配中对应 `'ghi'`。

```python
string = 'abc  def  ghi  jkl'
print(re.findall(r'(\w+)\s+\w+', string))

"""
>>> ['abc', 'ghi']
"""
```

若正则中有多个括号，返回一个列表，其中包含多个元组，每个元组都是一个匹配结果，元组中是括号匹配到的内容。

匹配结果同上，括号从左向右在第一次匹配中对应 `('abc def', 'abc', 'def')`，在第二次匹配中对应 `('ghi jkl', 'ghi', 'jkl')`。

```python
string = 'abc  def  ghi  jkl'
print(re.findall(r'((\w+)\s+(\w+))', string))

"""
>>> [('abc  def', 'abc', 'def'), ('ghi  jkl', 'ghi', 'jkl')]
"""
```

**re.Match**

可以观察到 `search()` 或 `match()` 返回都是同一个匹配对象 `re.Match`。它支持查看分组中的内容。

```python
string = 'Nice to meet you.'

res = re.search(r'(\w+) (\w+) (.*)', string)

# groups() 返回所有匹配的分组
print(res.groups())

# group() 返回匹配的分组，只有一个参数则返回字符串，多个参数返回元组
print(res.group())      # 相当于 group(0)，返回整个匹配的字符串
print(res.group(2))     # 通过分组编号获取对应内容
print(res.group(1, 3))

"""
>>> ('Nice', 'to', 'meet you.')
>>> Nice to meet you.
>>> to
>>> ('Nice', 'meet you.')
"""
```

**替换**

```python
import re

string = 'I love you'

pattern = re.compile(r'love')

res1 = pattern.sub('hate', string)
res2 = re.sub(pattern, 'hate', string)
res3 = re.sub(r'love', 'hate', string)

print(res1)
print(res2)
print(res3)

"""
>>> I hate you
>>> I hate you
>>> I hate you
"""
```

`repl` 还可以是一个函数

```python
import re

string = '我喜欢身高为 178 的女孩'
# 需求：在原本身高的基础上减去 10 并替换

pattern = re.compile(r'\d+')


def fun(obj):
    print(obj)  # obj 是获取的匹配对象
    h = int(obj.group())
    return str(h - 10)


res = pattern.sub(fun, string)

print(res)

"""
>>> <re.Match object; span=(7, 10), match='178'>
>>> 我喜欢身高为 168 的女孩
"""
```

**例子**

糗事百科

```python
import urllib.request
import re
import os
import time  # 下载停顿一下，避免服务器发现你是爬虫程序

# https://www.qiushibaike.com/

if not os.path.exists('糗事百科'):
    os.mkdir('糗事百科')


# 封装函数
def handle_request(url, headers, page):
    url = url + str(page) + '/'
    request = urllib.request.Request(url=url, headers=headers)
    return request


# <img src="//pic.qiushibaike.com/system/pictures/12146/121469975/medium/S7TBKM9U6IAKL91K.jpg" alt="">
def download_img(content, _path):
    pattern = re.compile(r'<div class="thumb">.*?<img src="(.*?)" .*?>', re.S)
    res = pattern.findall(content)
    # 遍历列表，一次下载图片
    for img_src in res:
        img_url = 'https:' + img_src
        filename = img_url.split('/')[-1]
        # 以图片 S7TBKM9U6IAKL91K.jpg 为文件名字
        # 以 / 切割 img_url，获取到最后一个
        filepath = _path + '/' + filename
        print("%s图片正在下载..." % filename)
        urllib.request.urlretrieve(img_url, filepath)
        time.sleep(0.1)


def main():
    url = 'https://www.qiushibaike.com/imgrank/page/'
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) '\
        'Chrome/58.0.3029.110 Safari/537.36 SE 2.X MetaSr 1.0',
    }
    start_page = int(input("请输入起始页码..."))
    end_page = int(input("请输入结束页码..."))
    for page in range(start_page, end_page+1):
        print("第%s页开始下载..." % page)
        # 生成请求对象
        request = handle_request(url, headers, page)
        # 发送请求
        content = urllib.request.urlopen(request).read().decode()
        # 解析内容
        _path = '糗事百科/page_' + str(page)
        if not os.path.exists(_path):
            os.mkdir(_path)
        download_img(content, _path)
        print("第%s页下载完毕\n\n" % page)
        time.sleep(2)


if __name__ == '__main__':
    main()
```

励志签名

```python
import urllib.request
import re

# http://www.yikexun.cn/lizhi/qianming/


def handle_request(url, page=None):
    if not page:
        url = url + str(page) + '.html'
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) '\
        'Chrome/58.0.3029.110 Safari/537.36 SE 2.X MetaSr 1.0',
    }
    request = urllib.request.Request(url=url, headers=headers)
    return request


def get_text(url):
    # 构建请求
    request = handle_request(url=url)
    content = urllib.request.urlopen(request).read().decode()
    pattern = re.compile(r'<div class="neirong">(.*?)</div>', re.S)
    # <div class="neirong"><ol class=" list-paddingleft-2" style="list-style-type: decimal;">...</ol></div>
    lt = pattern.findall(content)
    text = lt[0]
    # 去除所有图片
    pat = re.compile(r'<img .*?>')
    text = pat.sub('', text)  # 正则替换
    return text


def parse_content(content):
    pattern = re.compile(r'<h3><a href="(/lizhi/qianming/\d+.html)"><b>(.*?)</b></a></h3>')
    # <h3><a href="/lizhi/qianming/20190141201.html"><b>在自己的人生里做主角——激情人生激励自我经典语录</b></a></h3>
    list = pattern.findall(content)
    for i in list:
        title = i[1]
        url = 'http://www.yikexun.cn' + i[0]
        text = get_text(url)
        # 将 title 和text 写入 html 文件中
        string = '<h1>%s</h1><div>%s</div>' % (title, text)
        with open('励志签名.html', 'a', encoding='utf8') as fp:
            # 'w' 每次会清空，只能用 'a'，encoding 默认跟随系统，中文系统都是 gbk，utf8 网页都要写明
            fp.write(string)


def main():
    url = 'http://www.yikexun.cn/lizhi/qianming/list_50_'
    # http://www.yikexun.cn/lizhi/qianming/list_50_1.html
    start_page = int(input("请输入起始页码..."))
    end_page = int(input("请输入结束页码..."))
    for page in range(start_page, end_page+1):
        request = handle_request(url, page)
        content = urllib.request.urlopen(request).read().decode()
        # 解析内容
        parse_content(content)


if __name__ == '__main__':
    main()
```

## 5. Beautiful Soup

## 6. JsonPath

## 7. XPath

### 7.1 XPath 简介

**什么是 XML**

XML (Extensible Markup Language) 指可扩展标记语言，被设计用来存储和传输文本数据。

**XML 与 HTML 的不同**

- HTML 用来显示数据，XML 用来存储传输数据
- HTML 标签是固定的，XML 标签是自定义的
- XML 必须有且只有一个跟标签

**什么是 XPath**

XML 路径语言，最初用来搜寻 XML 文档，是一种路径表达式，但同样适用于 HTML 文档的搜索。

### 7.2 XPath 语法

**解析 XML**

```
nodename    选取 nodename 的所有子节点
/           从根节点选取
//          不考虑位置的选取
./          从当前节点往后选取
../         从父亲节点往后选取
@           选取属性
*           通配符

/store/book: 选取根节点 store 所有子节点 book
//book: 选取所有 book
/store//book: 选取 store 所有后代节点 book
/store/book[1]: store 下第一个 book
/store/book[last()]: store 下最后一个 book
/store/book[position()<3]: store 下前两个 book
/bookstore/book[price>35.00]: bookstore 中 price > 35.00 的所有 book
//title[@name]: 所有带有 name 属性的 title
//title[@name='li']: 所有 name 属性为 li 的 title
//title[@*]: 所有带属性的 title
```

**解析 HTML**

Chrome 中安装 *XPath Helper* 插件。chrome://extensions
Firefox 中安装 *XPath Checker* 插件。about:addons

```
属性定位
//input[@id='kw']
//input[@class='btn self-btn bg s_btn'] : 有多个 class 值必须全部写完

层级定位

索引定位
//div[@id='head_wrapper']/div[2]/a[1] : 索引从 1 开始
//div[@id='head_wrapper']//a[@class='toindex']

逻辑运算
//input[@type='text' and @name='wd']

模糊匹配
contains
    //input[contains(@class, 's_i')] : 有 class 属性，并且 class 中含有 s_i 的节点
    contains(text(), "爱") : 内容中含有“爱”字的节点，starts 没有这用法
starts-with
    //input[starts-with(@class, 's')] : 有 class 属性，并且属性以 s 开头

取文本
//div[@id='u_sp']/a[5]/text() : 获取节点内容
//div[@id='u_sp']/a//text() : 获取节点中所有不带标签的内容

直接将所有内容拼接起来
res = tree.xpath('//div[@class="song"]')
string = res[0].xpath('string(.)')
print(string.replace('\n', '').replace('\t', ''))

取属性
//div[@id='u_sp']/a[5]/@href
```

### 1.4 Python 中使用 XPath

**安装第三方库**

```
pip install lxml
```

**使用流程**

```py
# 导入 XPath 模块
from lxml import etree

# 将 HTML 文档生成对象
tree = etree.parse(filename)      # 本地
tree = etree.HTML(html_document)  # 网络

# 搜索文档
tree.xpath('...')
```

**例子**

## 8. PyQuery

### 8.1 PyQuery 简介

**什么是 PyQuery**

类似 jquery 的一个供 python 使用的 html 解析库，用法类似 bs4。

**安装**

```
pip install pyquery
```

### 8.2 采坑记录

**PyQuery 解析 HTML**

PyQuery 是利用 lxml 进行解析的，它们默认解析的都是 XML，导致用于解析 HTML 有时需要标明解析的方法。

空标签被自动闭合，导致 HTML 忽略

```python
from pyquery import PyQuery as pq

print(pq('test<a></a>').html())
print(pq('test<a></a>').html(method='html'))

"""
>>> test<a/>
>>> test<a></a>
"""
```

<style>
.bigbox {border: 1px solid black; padding: 10px; margin: 10px;}
</style>